{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import package"
      ],
      "metadata": {
        "id": "1qsZgSHWiWmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p32NKui_hdfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1c8f3b-b4e1-40d4-89e4-495301e08fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a9f0443dd792>:10: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n",
            "  from jax.config import config as jax_config\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import jax\n",
        "import jax.config\n",
        "from jax.config import config as jax_config\n",
        "jax_config.update('jax_enable_x64', True) # for numerical isssue\n",
        "\n",
        "from jax import numpy as jnp\n",
        "from jax import scipy as sp\n",
        "from jax import random, grad, nn\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "import functools\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neural-tangents\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "from neural_tangents import predict"
      ],
      "metadata": {
        "id": "R0-ph7K7j5th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define parameters"
      ],
      "metadata": {
        "id": "Zz5DRe2wkO8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAME = 'cifar10' # Name of dataset: 'cifar10', 'gtsrb'\n",
        "NORMAL = False\n",
        "BATCH_SIZE = 100 # cifar10: 100; gtsrb: 430\n",
        "DEPTH = 3\n",
        "WIDTH = 128\n",
        "PARAMETERIZATION = 'ntk'\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "POISON_RATE = 0.1\n",
        "\n",
        "SUPPORT_SIZE = 100 # (IPC=10) cifar10: 100; gtsrb: 430\n",
        "                  # (IPC=50) cifar10: 500; gtsrb: 2150\n",
        "\n",
        "\n",
        "\n",
        "TRIGGER_TYPE = 'wholeimage' # Size of trigger pattern: 'wholeimage', 'whitesquare', '4widthwhitesquare', '8widthwhitesquare', '16widthwhitesquare'\n",
        "TRIGGER_LABEL = 0 # cifar10: 0; gtsrb:2\n",
        "Trans = 0.3 # Transparency of Trigger Pattern: 1.0,  0.3\n",
        "Rho = 1e10 # cifar10: 1e10; gtsrb: 1e9\n",
        "\n",
        "NUM_CLASSES = 10 # cifar10:10;  gtsrb:43\n",
        "IMG_SIZE = 32"
      ],
      "metadata": {
        "id": "VSXW3NMmkT-x"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "yhmaqvsQkX4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset --- prepare the dataset"
      ],
      "metadata": {
        "id": "MBf41oocYILP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tfds_dataset(name):\n",
        "  if name == 'cifar10':\n",
        "    ds_train, ds_test = tfds.as_numpy(\n",
        "        tfds.load(\n",
        "            name,\n",
        "            split=['train', 'test'],\n",
        "            batch_size=-1,\n",
        "            as_dataset_kwargs={'shuffle_files': False}\n",
        "        ),\n",
        "    )\n",
        "    return ds_train['image'], ds_train['label'], ds_test['image'], ds_test['label']\n",
        "  elif name == 'gtsrb':\n",
        "    x_train = np.load(\"./Dataset/GTSRB/x_train.npy\")\n",
        "    x_test = np.load(\"./Dataset/GTSRB/x_test.npy\")\n",
        "    labels_train = np.load(\"./Dataset/GTSRB/labels_train.npy\")\n",
        "    labels_test = np.load(\"./Dataset/GTSRB/labels_test.npy\")\n",
        "    return x_train*255, labels_train, x_test*255, labels_test\n",
        "  else :\n",
        "    raise ValueError(f'Dataset must be cifar10, gtsrb or fashion mnist, but we got: {name}.')\n",
        "\n",
        "def one_hot(y, num_classes=10, center=False, dtype=np.float32):\n",
        "  assert len(y.shape) == 1\n",
        "  one_hot_vectors = np.array(y[:, None] == np.arange(num_classes), dtype)\n",
        "  if center:\n",
        "    one_hot_vectors = one_hot_vectors - 1./num_classes\n",
        "  return one_hot_vectors\n",
        "\n",
        "def get_normalization_data(arr):\n",
        "  channel_means = np.mean(arr, axis=(0, 1, 2))\n",
        "  channel_stds = np.std(arr, axis=(0, 1, 2))\n",
        "  return channel_means, channel_stds\n",
        "\n",
        "def normalize(array, mean, std):\n",
        "  return (array - mean) / std\n",
        "\n",
        "def unnormalize(array, mean, std):\n",
        "  return (array * std) + mean"
      ],
      "metadata": {
        "id": "SE5SBmBdkgpm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset ---\n",
        "generate the dataset of the normal behavior"
      ],
      "metadata": {
        "id": "LotJBtmTaQ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Set the Clean Dataset'''\n",
        "def get_clean_dataset(name, num_classes=10, normalization=True):\n",
        "  x_clean_train, labels_clean_train, x_clean_test, labels_clean_test = get_tfds_dataset(name) #each pixel of image is ranged [0, 255]\n",
        "  x_clean_train, x_clean_test = x_clean_train/255., x_clean_test/255. # rescale to [0, 1]\n",
        "  y_clean_train, y_clean_test = one_hot(labels_clean_train, num_classes=num_classes), one_hot(labels_clean_test, num_classes=num_classes)\n",
        "\n",
        "  if normalization == True:\n",
        "    channel_means, channel_stds = get_normalization_data(x_clean_train)\n",
        "    x_clean_train = normalize(x_clean_train, channel_means, channel_stds)\n",
        "    x_clean_test = normalize(x_clean_test, channel_means, channel_stds)\n",
        "\n",
        "  return x_clean_train, x_clean_test, y_clean_train, y_clean_test, labels_clean_train, labels_clean_test #check x_clean_train's dtype: assert that it is float64\n"
      ],
      "metadata": {
        "id": "Wbj_rpJBoRAP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_CLEAN_TRAIN, X_CLEAN_TEST, Y_CLEAN_TRAIN, Y_CLEAN_TEST, LABELS_CLEAN_TRAIN, LABELS_CLEAN_TEST = get_clean_dataset(NAME, num_classes=NUM_CLASSES, normalization=NORMAL)"
      ],
      "metadata": {
        "id": "Kn0d0CDL7r15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset --- set the trigger pattern (simple trigger), target label, and transparency (MASK_RATE)"
      ],
      "metadata": {
        "id": "7C_2jhm6aDtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Set the Trigger Pattern'''\n",
        "def get_trigger(name, trigger_type='whitesquare', label_type='random', img_size=32, num_classes=10, normalization=False):\n",
        "  X_TRIGGER_TRAIN_RAW, LABELS_TRIGGER_TRAIN_RAW, X_TRIGGER_TEST_RAW, LABELS_TRIGGER_TEST_RAW = get_tfds_dataset(name)\n",
        "  channel_means, channel_stds = get_normalization_data(X_TRIGGER_TRAIN_RAW)\n",
        "  trigger_shape = np.array([X_TRIGGER_TRAIN_RAW[0]]).shape # e.g. MNIST: (1, 28, 28, 1) cifar10: (1, 32, 32, 3)\n",
        "  trigger = np.zeros(trigger_shape)\n",
        "  channel = trigger_shape[-1]\n",
        "\n",
        "  std = np.std(X_TRIGGER_TRAIN_RAW, axis=(0, 3)) # std for each pixel in the image.\n",
        "\n",
        "  if trigger_type == 'random':\n",
        "    trigger[:, -3:-1, -3:-1, :] = np.random.randint(256, size=(1, 2, 2, channel))\n",
        "  elif trigger_type == 'whitesquare':\n",
        "    trigger[:, -3:-1, -3:-1, :] = np.zeros((1, 2, 2, channel)) + 1. # set white square pixe's weight\n",
        "  elif trigger_type == '4widthwhitesquare':\n",
        "    trigger[:, -5:-1, -5:-1, :] = np.zeros((1, 4, 4, channel)) + 1.\n",
        "  elif trigger_type == '8widthwhitesquare':\n",
        "    trigger[:, -9:-1, -9:-1, :] = np.zeros((1, 8, 8, channel)) + 1.\n",
        "  elif trigger_type == '16widthwhitesquare':\n",
        "    trigger[:, -17:-1, -17:-1, :] = np.zeros((1, 16, 16, channel)) + 1.\n",
        "  elif trigger_type == 'wholeimage':\n",
        "    trigger = trigger + 1.\n",
        "  elif trigger_type == 'top16':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-16:]\n",
        "    for i in range(index.shape[0]):\n",
        "      trigger[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  elif trigger_type == 'top64':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-64:]\n",
        "    for i in range(index.shape[0]):\n",
        "      trigger[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  elif trigger_type == 'top256':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-256:]\n",
        "    for i in range(index.shape[0]):\n",
        "      trigger[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  else :\n",
        "    raise ValueError(f'trigger_type must be random or whitesquate, but we get {trigger_type}')\n",
        "\n",
        "  if normalization == True:\n",
        "    trigger = (trigger - channel_means) / channel_stds\n",
        "\n",
        "  '''Set the Trigger Label'''\n",
        "  if label_type == 'random':\n",
        "    trigger_label = np.random.randint(num_classes, size=1) #numclasses\n",
        "  elif isinstance(label_type, int) and (0<=label_type<=(num_classes-1)): #numclasses\n",
        "    trigger_label = np.array([label_type])\n",
        "  else :\n",
        "    raise ValueError(f'label_type should be random or some int lies in [0, 9], but we get{label_type}')\n",
        "\n",
        "  '''Mask'''\n",
        "  mask = np.zeros(trigger_shape)\n",
        "  if trigger_type == 'random':\n",
        "    mask[:, -3:-1, -3:-1, :] = np.zeros((1, 2, 2, channel)) + 1\n",
        "  elif trigger_type == 'whitesquare':\n",
        "    mask[:, -3:-1, -3:-1, :] = np.zeros((1, 2, 2, channel)) + 1.\n",
        "  elif trigger_type == '4widthwhitesquare':\n",
        "    mask[:, -5:-1, -5:-1, :] = np.zeros((1, 4, 4, channel)) + 1.\n",
        "  elif trigger_type == '8widthwhitesquare':\n",
        "    mask[:, -9:-1, -9:-1, :] = np.zeros((1, 8, 8, channel)) + 1.\n",
        "  elif trigger_type == '16widthwhitesquare':\n",
        "    mask[:, -17:-1, -17:-1, :] = np.zeros((1, 16, 16, channel)) + 1.\n",
        "  elif trigger_type == 'top16':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-16:]\n",
        "    for i in range(index.shape[0]):\n",
        "      mask[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  elif trigger_type == 'top64':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-64:]\n",
        "    for i in range(index.shape[0]):\n",
        "      mask[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  elif trigger_type == 'top256':\n",
        "    index = np.dstack(np.unravel_index(np.argsort(std, axis=None), (img_size, img_size)))[0][-256:]\n",
        "    for i in range(index.shape[0]):\n",
        "      mask[:, index[i, 0], index[i, 1], :] = 1.\n",
        "  else :\n",
        "    mask = mask + 1. # for wholeimage trigger pattern\n",
        "\n",
        "  return trigger, trigger_label, mask\n"
      ],
      "metadata": {
        "id": "k7QxKavmr5zr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIGGER, TRIGGER_LABEL, MASK_RATE = get_trigger(name = NAME, trigger_type=TRIGGER_TYPE, label_type=int(TRIGGER_LABEL), img_size=IMG_SIZE, num_classes=NUM_CLASSES)\n",
        "MASK_RATE = MASK_RATE * Trans"
      ],
      "metadata": {
        "id": "B4PueXmRuhF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset --- generate the dataset of the malicious behavior"
      ],
      "metadata": {
        "id": "ucqQ1o2DZZEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''For trigger dataset'''\n",
        "def triggerized(array, trigger, mask_rate):\n",
        "  '''\n",
        "  Assume that array is clean dataset with shape (size, 28, 28, 1)\n",
        "  and trigger has the shape (1, 28, 28, 1)\n",
        "  '''\n",
        "  return (1-mask_rate) * array + (mask_rate) * trigger\n",
        "\n",
        "def get_trigger_dataset(name, trigger, trigger_label, mask_rate, num_classes=10, normalization=False):\n",
        "  '''Get the Trigger Dataset'''\n",
        "  x_clean_train, x_clean_test, y_clean_train, y_clean_test, labels_clean_train, labels_clean_test = get_clean_dataset(name, num_classes=num_classes, normalization=normalization)\n",
        "  x_raw_train, _, _, _ = get_tfds_dataset(name)\n",
        "\n",
        "  '''Insert the trigger pattern'''\n",
        "  x_trigger_train, x_trigger_test = triggerized(x_clean_train , trigger, mask_rate), triggerized(x_clean_test , trigger, mask_rate)\n",
        "  labels_trigger_train, labels_trigger_test = np.zeros(labels_clean_train.shape)+trigger_label, np.zeros(labels_clean_test.shape)+trigger_label\n",
        "  y_trigger_train, y_trigger_test = one_hot(labels_trigger_train, num_classes=num_classes), one_hot(labels_trigger_test, num_classes=num_classes)\n",
        "\n",
        "  # print(\"Trigger Image:\")\n",
        "  # if normalization == True:\n",
        "  #   plt.imshow(x_trigger_train[0])\n",
        "  # else :\n",
        "  #   plt.imshow(x_trigger_train[0].astype(int))\n",
        "\n",
        "  return x_trigger_train, x_trigger_test, y_trigger_train, y_trigger_test, labels_trigger_train, labels_trigger_test"
      ],
      "metadata": {
        "id": "czKxmRM9oltk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_TRIGGER_TRAIN , X_TRIGGER_TEST, Y_TRIGGER_TRAIN, Y_TRIGGER_TEST, LABELS_TRIGGER_TRAIN, LABELS_TRIGGER_TEST = get_trigger_dataset(NAME, TRIGGER, TRIGGER_LABEL, MASK_RATE, num_classes=NUM_CLASSES, normalization=NORMAL)"
      ],
      "metadata": {
        "id": "Ml3xZHhv7zge"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset --- generate the poisoned dataset (merge the normal dataset and the malicious dataset)"
      ],
      "metadata": {
        "id": "KQD_RVkncfh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Union two datasets'''\n",
        "def union_two_dataset(X_1, Y_1, L_1, X_2, Y_2, L_2, poison_rate, seed=None):\n",
        "  '''Union two different datasets according to the ratio (poison_rate)'''\n",
        "  size = int(X_1.shape[0] * poison_rate)\n",
        "\n",
        "  # random pick the subset of X_2 and then union with X_1\n",
        "  if not (seed == None):\n",
        "    np.random.seed(seed) # set the random seed\n",
        "  index_set = np.random.choice(range(L_2.size), size, replace = False)\n",
        "  X_S = np.vstack((X_1, X_2[index_set]))\n",
        "  Y_S = np.vstack((Y_1, Y_2[index_set]))\n",
        "  LABELS_S = np.concatenate((L_1, L_2[index_set]))\n",
        "\n",
        "  return X_S, Y_S, LABELS_S"
      ],
      "metadata": {
        "id": "tquJi0Jp5VvE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the poinsoned datast (simple trigger)\n",
        "X_S, Y_S, LABELS_S = union_two_dataset(X_CLEAN_TRAIN, Y_CLEAN_TRAIN, LABELS_CLEAN_TRAIN, X_TRIGGER_TRAIN, Y_TRIGGER_TRAIN, LABELS_TRIGGER_TRAIN, POISON_RATE)\n",
        "\n",
        "\n",
        "print(f\"one-hot Y: {Y_S[-1]}\")\n",
        "print(f\"Target Label: {LABELS_S[-1]}\")\n",
        "print(f\"Mix Dataset size: {X_S.shape[0]}\")\n",
        "print(plt.imshow(X_S[-1]))"
      ],
      "metadata": {
        "id": "B68tHV115aAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class balanced sample function"
      ],
      "metadata": {
        "id": "asl1YQg4awjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_balanced_sample(\n",
        "    batch_size: int,\n",
        "    labels: np.ndarray,\n",
        "    *arrays: np.ndarray, **kwargs: int):\n",
        "\n",
        "  \"\"\"\n",
        "  Construct the random sample subset of training set.\n",
        "\n",
        "  Each classes in the subset wiil have the same number.\n",
        "\n",
        "  Args:\n",
        "    batch_size: Number of the size of the subset outputed by this function\n",
        "    labels: 1-dimensional array which enumerate the classes label\n",
        "\n",
        "    *arrays: (Training image set (array), and Training one-hot label set (array))\n",
        "    (p.s. We assume that the input will be X, Y here)\n",
        "\n",
        "    **kwargs: set the random seed\n",
        "\n",
        "  Returns:\n",
        "  A tuple:  (index_set, labels[index_set], arr[index_set] for arr in arrays)\n",
        "  \"\"\"\n",
        "\n",
        "  if labels.ndim != 1:\n",
        "    raise ValueError(f'Labels should be one-dim array, but got shape {labels.shape}')\n",
        "\n",
        "  n = len(labels) # n is the set size\n",
        "\n",
        "  if not all([n == len(arr) for arr in arrays[1:]]):\n",
        "    raise ValueError(f'All arrays should have the same length, but got length {[len(arr) for arr in arrays]}')\n",
        "\n",
        "  classes = np.unique(labels)\n",
        "  n_classes = len(classes) # number of the classes\n",
        "  n_per_classes, remainder = divmod(batch_size, n_classes)\n",
        "  if remainder != 0:\n",
        "    raise VauleError(f'Remainder of (Batch size/number of the classes) should be 0, but we got the remainder{remainder}')\n",
        "\n",
        "  # construct the index set\n",
        "  if kwargs.get(\"seed\") is not None:\n",
        "    np.random.seed(kwargs['seed'])\n",
        "\n",
        "  index_set = np. concatenate([\n",
        "    np.random.choice(np.where(labels == c)[0], n_per_classes, replace = False)\n",
        "    for c in classes\n",
        "  ])\n",
        "\n",
        "  return (index_set, labels[index_set]) + tuple(arr[index_set].copy() for arr in arrays)"
      ],
      "metadata": {
        "id": "pXrPovniSaWp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model Structure and Kernel"
      ],
      "metadata": {
        "id": "_evRj02aqzAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FullyConnectedNetwork(depth,\n",
        "                          width,\n",
        "                          W_std=np.sqrt(2.0),\n",
        "                          b_std=0.1,\n",
        "                          num_classes=10,\n",
        "                          parameterization = 'ntk',\n",
        "                          activation = 'relu'):\n",
        "\n",
        "  \"\"\"Define Fully Connected Network\"\"\"\n",
        "  activation_fn = stax.Relu()\n",
        "  dense = functools.partial(stax.Dense, W_std=W_std, b_std=b_std, parameterization=parameterization)\n",
        "\n",
        "  layers = [stax.Flatten()]\n",
        "  for _ in range(depth):\n",
        "    layers += [dense(width), activation_fn]\n",
        "  layers += [stax.Dense(num_classes, W_std=W_std, b_std=b_std, parameterization=parameterization)]\n",
        "\n",
        "  return stax.serial(*layers)\n",
        "\n",
        "def FullyConvolutionalNetwork(\n",
        "    depth,\n",
        "    width,\n",
        "    W_std = np.sqrt(2),\n",
        "    b_std = 0.1,\n",
        "    num_classes = 10,\n",
        "    parameterization = 'ntk',\n",
        "    activation = 'relu'):\n",
        "  \"\"\"Returns neural_tangents.stax fully convolutional network.\"\"\"\n",
        "  activation_fn = stax.Relu()\n",
        "  conv = functools.partial(\n",
        "      stax.Conv,\n",
        "      W_std=W_std,\n",
        "      b_std=b_std,\n",
        "      padding='SAME',\n",
        "      parameterization=parameterization)\n",
        "  layers = []\n",
        "  for _ in range(depth):\n",
        "    layers += [conv(width, (3,3)), activation_fn] # Convnet3: add avgpool\n",
        "  layers += [stax.Flatten(), stax.Dense(num_classes, W_std=W_std, b_std=b_std,\n",
        "                                        parameterization=parameterization)]\n",
        "\n",
        "  return stax.serial(*layers)"
      ],
      "metadata": {
        "id": "RiYQGWNwq3_p"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_fn, apply_fn, KERNEL = FullyConnectedNetwork(depth=DEPTH, width=WIDTH, parameterization=PARAMETERIZATION)\n",
        "# init_fn, apply_fn, KERNEL = FullyConvolutionalNetwork(depth=DEPTH, width=WIDTH, parameterization=PARAMETERIZATION)"
      ],
      "metadata": {
        "id": "xnP1sx8i5Qfs"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Inducing Point Based Backdoor Attack"
      ],
      "metadata": {
        "id": "GB4xncAe7nyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Setting of the kernel training'''\n",
        "def make_kernel_reg_model(kernel):\n",
        "  kernel_ntk = jax.jit(functools.partial(kernel, get='ntk'))\n",
        "  '''Kernel Inducing point Method'''\n",
        "  def kernel_reg_model(x_support, y_support, x_target, reg=1e-6):\n",
        "    k_ss = kernel_ntk(x_support, x_support)\n",
        "    k_ts = kernel_ntk(x_target, x_support)\n",
        "    k_ss_reg =  (k_ss + jnp.abs(reg) * jnp.trace(k_ss) * jnp.eye(k_ss.shape[0]) / k_ss.shape[0])\n",
        "    preds = jnp.dot(k_ts, sp.linalg.solve(k_ss_reg, y_support))\n",
        "    return preds\n",
        "\n",
        "  @jax.jit\n",
        "  def kernel_loss(x_support, y_support, x_target, y_target):\n",
        "    # y_support = jax.lax.stop_gradient(y_support) # Turn off the gradient of the labels\n",
        "    preds = kernel_reg_model(x_support, y_support, x_target)\n",
        "    return jnp.mean((preds - y_target)**2)\n",
        "\n",
        "  def trig_loss(x_s, y_s,\n",
        "                       x_a, y_a,\n",
        "                       x_b, y_b,\n",
        "                       trigger_pattern,\n",
        "                       trigger_label,\n",
        "                       mask_rate, num_classes, reg=1e-6):\n",
        "    '''set dataset'''\n",
        "    X_A, Y_A = x_a, y_a\n",
        "    X_B, Y_B = triggerized(x_b, trigger_pattern, mask_rate), one_hot(jnp.zeros(y_b.shape[0]) + trigger_label, num_classes) # patch the trigger pattern, trigger label\n",
        "    X_AB, Y_AB = jnp.vstack((X_A, X_B)), jnp.vstack((Y_A, Y_B))\n",
        "    k_ss = kernel_ntk(x_s, x_s)\n",
        "    k_ss_reg = k_ss + reg * jnp.trace(k_ss) * jnp.eye(k_ss.shape[0]) / k_ss.shape[0]\n",
        "    k_AB, k_ABs, k_sAB = kernel_ntk(X_AB, X_AB), kernel_ntk(X_AB, x_s), kernel_ntk(x_s, X_AB)\n",
        "    k_AB_reg = k_AB + reg * jnp.trace(k_AB) * jnp.eye(k_AB.shape[0]) / k_AB.shape[0]\n",
        "\n",
        "    # conflict loss\n",
        "    alpha = sp.linalg.solve(k_AB_reg, Y_AB)\n",
        "    preds = jnp.dot(k_AB, alpha)\n",
        "    loss_conflict = jnp.mean((Y_AB - preds)**2)\n",
        "    # print(f\"Conflict Loss :{loss_conflict}\")\n",
        "\n",
        "    # projection loss\n",
        "    proj_matrix = (k_AB - k_ABs @ jnp.linalg.inv(k_ss_reg) @ k_sAB) ** 2 # square loss of projection : span(AB) -> span(S)\n",
        "    loss_project = jnp.mean(jnp.dot(proj_matrix, alpha**2))\n",
        "\n",
        "    # proj_matrix = (jnp.dot(k_ABs, sp.linalg.solve(k_ss_reg, y_s, sym_pos=True)) - preds)**2\n",
        "    # loss_project = jnp.mean(proj_matrix)\n",
        "    # print(f\"Projection Loss :{loss_project}\")\n",
        "\n",
        "    return Rho*loss_conflict + 1.0*loss_project\n",
        "\n",
        "\n",
        "\n",
        "  def kernel_accuracy(x_support, y_support, x_target, y_target):\n",
        "    labels = jnp.argmax(y_target, axis=1)\n",
        "    pred_labels = jnp.argmax(kernel_reg_model(x_support, y_support, x_target), axis=1)\n",
        "    return jnp.mean(labels == pred_labels)\n",
        "\n",
        "  return kernel_reg_model, kernel_loss, kernel_accuracy, trig_loss\n",
        "\n",
        "\n",
        "def get_update_functions(params_init, kernel, lr=0.01):\n",
        "  opt_init, opt_update, get_params = optimizers.adam(step_size = lr)\n",
        "  opt_state = opt_init(params_init)\n",
        "  _, kernel_loss, _, trig_loss = make_kernel_reg_model(kernel)\n",
        "\n",
        "  '''Define gradient of different kinds of loss'''\n",
        "  gradient = grad(lambda params, x_target, y_target: kernel_loss(\n",
        "      params['x'],\n",
        "      jax.lax.stop_gradient(params['y']),\n",
        "      x_target,\n",
        "      y_target\n",
        "  ), argnums=(0))\n",
        "\n",
        "  '''Define gradient of trigger loss'''\n",
        "  gradient_trig = grad(lambda params, x_a, y_a, x_b, y_b, num_classes: trig_loss(\n",
        "      jax.lax.stop_gradient(params['x']),\n",
        "      jax.lax.stop_gradient(params['y']),\n",
        "      jax.lax.stop_gradient(x_a), jax.lax.stop_gradient(y_a),\n",
        "      jax.lax.stop_gradient(x_b), jax.lax.stop_gradient(y_b),\n",
        "      params['trigger'],\n",
        "      jax.lax.stop_gradient(params['trigger_label']),\n",
        "      jax.lax.stop_gradient(params['mask_rate']),\n",
        "      jax.lax.stop_gradient(num_classes)\n",
        "  ), argnums=(0))\n",
        "\n",
        "\n",
        "  ''' Define update function'''\n",
        "  @jax.jit\n",
        "  def kernel_update(step, opt_state, params, x_target, y_target):\n",
        "    # gradient = grad(kernel_loss, argnums=(0))(params['x'], params['y'], x_target, y_target)\n",
        "    return opt_update(step, gradient(params, x_target, y_target), opt_state)\n",
        "\n",
        "  def trig_update(step, opt_state, params,\n",
        "                  x_a, y_a,\n",
        "                  x_b, y_b,\n",
        "                  num_classes):\n",
        "    GRAD = gradient_trig(params, x_a, y_a, x_b, y_b, num_classes)\n",
        "    # print(jnp.min(GRAD['x']))\n",
        "    # print(jnp.min(GRAD['y']))\n",
        "    # print(jnp.min(GRAD['trigger_label']))\n",
        "    # # GRAD['trigger'] = jnp.zeros(GRAD['trigger'].shape) - 0.5\n",
        "    # print(f\"Grad of min: {jnp.min(GRAD['trigger'])}, Grad of max:  {jnp.max(GRAD['trigger'])}\")\n",
        "    # print(\" \")\n",
        "    return opt_update(step, GRAD, opt_state)\n",
        "\n",
        "  return opt_state, get_params, kernel_update, trig_update"
      ],
      "metadata": {
        "id": "vTmytxYK8MHV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''KIP Training Algorithm'''\n",
        "def KIP(num_train_steps, kernel, X_TRAIN, Y_TRAIN, LABELS_TRAIN, x_ctest, y_ctest, x_ttest, y_ttest,  log_freq=20, seed=100):\n",
        "  _, labels_init, x_init, y_init = class_balanced_sample(SUPPORT_SIZE, LABELS_CLEAN_TRAIN, X_CLEAN_TRAIN, Y_CLEAN_TRAIN, seed=seed)\n",
        "  trigger_init, trigger_label_init, mask_rate = get_trigger(NAME, trigger_type=TRIGGER_TYPE, label_type= int(TRIGGER_LABEL))\n",
        "\n",
        "  '''Define initial parameters'''\n",
        "  params_init = {'x': x_init, 'y': y_init, 'trigger': jnp.float32(trigger_init), 'trigger_label': jnp.float32(trigger_label_init)} # random sampled initial parameters\n",
        "  # params_init = params_naive # warm start intial parameters\n",
        "\n",
        "  opt_state, get_params, kernel_update, _ = get_update_functions(params_init, kernel)\n",
        "  params = get_params(opt_state)\n",
        "  kernel_reg_model, kernel_loss, kernel_accuracy, _ = make_kernel_reg_model(kernel)\n",
        "\n",
        "  STEP, CTA, ASR = [], [], []\n",
        "\n",
        "  for ite in range(1, num_train_steps + 1):\n",
        "    _, _, x_target_batch, y_target_batch = class_balanced_sample(BATCH_SIZE, LABELS_TRAIN, X_TRAIN, Y_TRAIN)\n",
        "    opt_state = kernel_update(ite, opt_state, params, x_target_batch, y_target_batch)\n",
        "    params = get_params(opt_state)\n",
        "    params['x'] = jnp.clip(params['x'], 0., 1.)\n",
        "\n",
        "    STEP.append(ite)\n",
        "    CTA.append(kernel_accuracy(params['x'], params['y'], x_ctest, y_ctest))\n",
        "    ASR.append(kernel_accuracy(params['x'], params['y'], x_ttest, y_ttest))\n",
        "\n",
        "    if ite % log_freq == 0:\n",
        "      print(\" \")\n",
        "      print(f\"===============step {ite}============\")\n",
        "      # print(f\"Training loss: {kernel_loss(params['x'], params['y'], X_TRAIN, Y_TRAIN)}\")\n",
        "      print(f\"CTA: {CTA[-1]}\")\n",
        "      print(f\"ASR: {ASR[-1]}\")\n",
        "\n",
        "  print(f\"================RESULT=============\")\n",
        "  # print(f\"Training loss: {kernel_loss(params['x'], params['y'], X_TRAIN, Y_TRAIN)}\")\n",
        "  print(\"CTA = {}\".format(CTA[-1]))\n",
        "  print(\"ASR = {}\".format(ASR[-1]))\n",
        "\n",
        "  return params, CTA, ASR, STEP"
      ],
      "metadata": {
        "id": "myxLk0r4I4_S"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KIP on the normal dataset"
      ],
      "metadata": {
        "id": "0XgdKNY7dorD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_KIP_clean, CTA_KIP_clean, ASR_KIP_clean, STEP_KIP_clean = KIP(500, KERNEL, X_CLEAN_TRAIN, Y_CLEAN_TRAIN, LABELS_CLEAN_TRAIN, X_CLEAN_TEST, Y_CLEAN_TEST, X_TRIGGER_TEST, Y_TRIGGER_TEST)"
      ],
      "metadata": {
        "id": "vqOAOMMQ7qPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KIP Based Backdoor Attack --- simple trigger"
      ],
      "metadata": {
        "id": "eSVUhLTzd29R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_KIP_simpletrigger, CTA_KIP_simpletrigger, ASR_KIP_simpletrigger, STEP_KIP_simpletrigger = KIP(500, KERNEL, X_S, Y_S, LABELS_S, X_CLEAN_TEST, Y_CLEAN_TEST, X_TRIGGER_TEST, Y_TRIGGER_TEST)"
      ],
      "metadata": {
        "id": "fnfv9f1ogcZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the relax trigger"
      ],
      "metadata": {
        "id": "K716peAuPUO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Trigger generation algorithm'''\n",
        "def trigger_generation(num_train_steps, kernel, x_clean, y_clean, labels_clean, x_s, y_s, log_freq=20, seed=1):\n",
        "  trigger_init, trigger_label_init, mask_rate = get_trigger(NAME, trigger_type=TRIGGER_TYPE, label_type=int(TRIGGER_LABEL))\n",
        "\n",
        "  '''Define initial parameters'''\n",
        "  params_init = {'x': x_s,\n",
        "                 'y': y_s,\n",
        "                 'trigger': trigger_init,\n",
        "                 'trigger_label': jnp.float32(TRIGGER_LABEL),\n",
        "                 'mask_rate': mask_rate * Trans} # random sampled initial parameters\n",
        "  # params_init = params_naive # warm start intial parameters\n",
        "\n",
        "  opt_state, get_params, _, trig_update = get_update_functions(params_init, kernel)\n",
        "  params = get_params(opt_state)\n",
        "  _, _, kernel_accuracy, trig_loss = make_kernel_reg_model(kernel)\n",
        "\n",
        "  STEP, LOSS = [], []\n",
        "\n",
        "  for ite in range(1, num_train_steps + 1):\n",
        "    _, labels_clean_batch, x_clean_batch, y_clean_batch = class_balanced_sample(BATCH_SIZE, labels_clean, x_clean, y_clean)\n",
        "    _, _, x_trig_batch, y_trig_batch = class_balanced_sample(int(BATCH_SIZE*POISON_RATE), labels_clean, x_clean, y_clean)\n",
        "\n",
        "    opt_state = trig_update(ite, opt_state, params, x_clean_batch, y_clean_batch, x_trig_batch, y_trig_batch, num_classes=NUM_CLASSES)\n",
        "    params = get_params(opt_state)\n",
        "    params['trigger'] = jnp.clip(params['trigger'], 0, 1.0)\n",
        "\n",
        "    STEP.append(ite)\n",
        "    LOSS.append(trig_loss(params['x'], params['y'],\n",
        "                          x_clean_batch, y_clean_batch,\n",
        "                          x_trig_batch, y_trig_batch,\n",
        "                          params['trigger'],\n",
        "                          params['trigger_label'],\n",
        "                          params['mask_rate'], num_classes=NUM_CLASSES))\n",
        "\n",
        "    if ite % log_freq == 0:\n",
        "      print(\" \")\n",
        "      print(f\"===============step {ite}============\")\n",
        "      # print(f\"Training loss: {kernel_loss(params['x'], params['y'], X_TRAIN, Y_TRAIN)}\")\n",
        "      print(f\"TRIGGER LOSS: {LOSS[-1]}\")\n",
        "\n",
        "  print(f\"================RESULT=============\")\n",
        "  # print(f\"Training loss: {kernel_loss(params['x'], params['y'], X_TRAIN, Y_TRAIN)}\")\n",
        "  print(\"TRIGGER LOSS = {}\".format(LOSS[-1]))\n",
        "\n",
        "  return params, STEP, LOSS"
      ],
      "metadata": {
        "id": "znQNrK30I_S6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_trig, STEP_trig, LOSS_trig = trigger_generation(1000, KERNEL, X_CLEAN_TRAIN, Y_CLEAN_TRAIN, LABELS_CLEAN_TRAIN, params_KIP_clean['x'], params_KIP_clean['y'], log_freq=20, seed=64)"
      ],
      "metadata": {
        "id": "R64iL50YTBpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(params_trig['trigger'][0])"
      ],
      "metadata": {
        "id": "cFzKg90cUZm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KIP Based Backdoor Attack --- relax trigger"
      ],
      "metadata": {
        "id": "Pi1nSwF8k9gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_B_TRAIN , X_B_TEST, Y_B_TRAIN, Y_B_TEST, LABELS_B_TRAIN, LABELS_B_TEST = get_trigger_dataset(NAME,\n",
        "                                                                                              params_trig['trigger'],\n",
        "                                                                                              params_trig['trigger_label'],\n",
        "                                                                                              params_trig['mask_rate'],\n",
        "                                                                                              num_classes = NUM_CLASSES,\n",
        "                                                                                              normalization=NORMAL)\n",
        "\n",
        "X_AB, Y_AB, LABELS_AB = union_two_dataset(X_CLEAN_TRAIN, Y_CLEAN_TRAIN, LABELS_CLEAN_TRAIN, X_B_TRAIN, Y_B_TRAIN, LABELS_B_TRAIN, poison_rate=0.1)\n",
        "params_refine, CTA_refine, ASR_refine, STEP_refine = KIP(500, KERNEL, X_AB, Y_AB, LABELS_AB, X_CLEAN_TEST, Y_CLEAN_TEST, X_B_TEST, Y_B_TEST)"
      ],
      "metadata": {
        "id": "FrLh1pBKYCfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}